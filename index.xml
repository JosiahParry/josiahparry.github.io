<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Josiah Parry on Josiah Parry</title>
    <link>/</link>
    <description>Recent content in Josiah Parry on Josiah Parry</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2018</copyright>
    <lastBuildDate>Sun, 15 Oct 2017 00:00:00 -0400</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>US Representation: Part I</title>
      <link>/post/us-representation-i/</link>
      <pubDate>Sun, 25 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/us-representation-i/</guid>
      <description>


&lt;p&gt;Before the United States created the Constitution, something called the &lt;a href=&#34;https://history.state.gov/milestones/1776-1783/articles&#34;&gt;&lt;em&gt;Articles of Confederation&lt;/em&gt;&lt;/a&gt; defined what the US Government would look like. It was the first attempt at creating some sort of agreement between the 13 original states to form a central government. In the end, the Articles of Confederation made the new central government too weak to accomplish anything. Then, in 1787 representatives from each state met in Philadelphia to entirely scrap the Articles of Confederation in a meeting that became known as the &lt;a href=&#34;https://history.state.gov/milestones/1784-1800/convention-and-ratification&#34;&gt;Constitutional Convention&lt;/a&gt;. They would then end up creating the Constitution of the United States of America which we all know today.&lt;/p&gt;
&lt;p&gt;During this time, there were three main issues at hand. Representatives of the convention sought to give each state enough autonomy to function independently. They engaged in heated debated about how much power each state should be given and eventually, the issue of slavery — (3) how would slaves be counted for tax and representation purposes?&lt;/p&gt;
&lt;p&gt;The debate surrounding how much power (or representation) that would be given to each state in the new government was the source of much rancour at the convention. There were two leading ideas that addressed this problem. One of which was that each state would have an equal say regardless of its physical size or the number of people within it. The other was that each state would have power relative to their total population.&lt;/p&gt;
&lt;p&gt;These ideas were presented as the &lt;a href=&#34;https://en.wikipedia.org/wiki/New_Jersey_Plan&#34;&gt;&lt;em&gt;New Jersey Plan&lt;/em&gt;&lt;/a&gt; and the &lt;a href=&#34;https://en.wikipedia.org/wiki/Virginia_Plan&#34;&gt;&lt;em&gt;Virginia Plan&lt;/em&gt;&lt;/a&gt;. James Madison drafted the Virginia Plan which also would be known as the “large-state-plan” and was intended to introduce proportional representation—effectively giving states with the most people the most power.&lt;/p&gt;
&lt;p&gt;Small states, feeling threatened by this, introduced the New Jersey plan. The New Jersey plan was an attempt to level the playing field between small and big states. The New Jersey plan would give each state 1 vote in the new government and would allow states like Delaware to have as much weight in votes as big states.&lt;/p&gt;
&lt;p&gt;In a move that would be known as the &lt;a href=&#34;https://www.senate.gov/artandhistory/history/minute/A_Great_Compromise.htm&#34;&gt;Great Compromise&lt;/a&gt; (or the Connecticut compromise), the representatives from Connecticut (a medium sized state) suggested that both ideas be put into effect. This idea created what is called a “bicameral legislature”—a legislative (law-making) body with two parts.&lt;/p&gt;
&lt;p&gt;One part of the new government would provide equal representation for each of the states. This became known as the &lt;em&gt;Senate&lt;/em&gt; which today has two representatives for each state. The other part of the new government became the &lt;em&gt;House of Representatives&lt;/em&gt; (also known more generally as “Congress”). This new body gave each state &lt;em&gt;“one [representative] for every Thirty Thousand [people]”&lt;/em&gt;. But as the US population grew so did the number of representatives in Washington DC. Eventually rules had to change to prevent the number of representatives from getting any more out of hand . Today, the House of Representatives has 435 congresspeople (more &lt;a href=&#34;https://en.wikipedia.org/wiki/United_States_congressional_apportionment#Past_increases&#34;&gt;here&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;Though the Connecticut compromise was an act of genius, it also implemented one of most reprehensible policies in US history: &lt;em&gt;the 3/5ths compromise&lt;/em&gt;. Slavery was the largest economic driving force in Southern states created by a seemingly endless supply of cost-free labor. Looking at the prospect of proportional representation, large slave owning states wanted each slave to be counted towards their population. For reference slaves composed 43% of the population of South Carolina, 41.6% of Virginia, 35.5% of Georgia, and 32% of North Carolina.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;/post/2018-11-25-representation-i_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt;&lt;/p&gt;
&lt;p&gt;But there is one major catch, slaves were treated as property and not people. They were not given the same “inalienable rights” as everyone else. This was seen as unfair because slaves would not &lt;em&gt;actually&lt;/em&gt; be &lt;em&gt;represented&lt;/em&gt; in congress. If only free people were to be counted, that would punish slave holding states and empower Northern states.&lt;/p&gt;
&lt;p&gt;Eventually representatives of the convention came to a solution, one that still haunts our country until this very day. It was decided that 3 out of every 5 slaves would be counted for representation purposes (called apportionment). This compromise gave Southern states more representative power than their free population actually dictated.&lt;/p&gt;
&lt;p&gt;This compromise created the government that we have today. It paved the foundation for the Senate and the House of Representatives. The effects of this decision are being seen today. During the time of the Connecticut Compromise the United States was still a fledgling rural nation. Today, we are a leader in industry, play an outsized role in global economics, and we have seen an enormous push to city and suburban areas. This compromise is demonstrating an increasing rural bias in American politics.&lt;/p&gt;
&lt;p&gt;In a following post I will examine the implications of increasing urbanization on governing and legislation in the United States.&lt;/p&gt;
&lt;p&gt;If you still feel like you want more, check out &lt;a href=&#34;https://www.youtube.com/watch?v=kCCmuftyj8A&#34;&gt;this&lt;/a&gt; fun youtube video on constitutional compromises.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Introducing: Letters to a layperson</title>
      <link>/post/introducing-letters-to-a-layperson/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-letters-to-a-layperson/</guid>
      <description>&lt;p&gt;I have been in the world of academia for nearly five years now. During this time I’ve read countless &lt;em&gt;scholarly&lt;/em&gt; journal articles that I’ve struggled to wrap my head around. The academic language is riddled with obfuscating words like “milieux” and “nexus” which are often used to explain relatively simple concepts in a not so simple language. I’ve had to train myself to understand the academic language and translate it to regular people (layperson) speak.&lt;/p&gt;

&lt;p&gt;The academic dialect is often associated with the &amp;ldquo;elitist media” (see &lt;a href=&#34;https://chomsky.info/199710__/&#34; target=&#34;_blank&#34;&gt;Chomsky&lt;/a&gt;) which has recently been blamed for creating a strong divide in American politics—as we’ve seen since the beginning of the 2016 presidential primaries. Many words, phrases, and ideas have been shrouded by this language barrier.  I have been trying to break down this barrier for myself for years now. I feel like I’ve only made a small dent. I have been trying to educate myself, a layperson, on these phrases and concepts.&lt;/p&gt;

&lt;p&gt;As an undergraduate student I studied sociology and anthropology, but I found that I was enamored with economics, political science, urban theory, data science, psychology, and other disciplines. Across these fields there are identical concepts represented by different words or phrases—an ever frustrating thing. This is a barrier to understanding these fields. You must know certain ideas, words, and histories to understand the content.&lt;/p&gt;

&lt;p&gt;I have been collecting notes on these ideas and often revisit them to remind myself of what they are, what they mean, and why they exist. These notes were created for a myself, a layperson.&lt;/p&gt;

&lt;p&gt;In this series of forthcoming posts, I will write about concepts that I wish I knew better in a language that I can understand. I call this collection of posts &lt;em&gt;Letters To a Layperson&lt;/em&gt;, inspired by the phenomenal book &lt;em&gt;Letters to a Young Contrarian&lt;/em&gt; by Christopher Hitchens.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Letters to a layperson</title>
      <link>/project/letters/</link>
      <pubDate>Thu, 15 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/letters/</guid>
      <description>&lt;p&gt;I have been in the world of academia for nearly five years now. During this time I’ve read countless &lt;em&gt;scholarly&lt;/em&gt; journal articles that I’ve struggled to wrap my head around. The academic language is riddled with obfuscating words like “milieux” and “nexus” which are often used to explain relatively simple concepts in a not so simple language. I’ve had to train myself to understand the academic language and translate it to regular people (layperson) speak.&lt;/p&gt;

&lt;p&gt;The academic dialect is often associated with the &amp;ldquo;elitist media” (see &lt;a href=&#34;https://chomsky.info/199710__/&#34; target=&#34;_blank&#34;&gt;Chomsky&lt;/a&gt;) which has recently been blamed for creating a strong divide in American politics—as we’ve seen since the beginning of the 2016 presidential primaries. Many words, phrases, and ideas have been shrouded by this language barrier.  I have been trying to break down this barrier for myself for years now. I feel like I’ve only made a small dent. I have been trying to educate myself, a layperson, on these phrases and concepts.&lt;/p&gt;

&lt;p&gt;As an undergraduate student I studied sociology and anthropology, but I found that I was enamored with economics, political science, urban theory, data science, psychology, and other disciplines. Across these fields there are identical concepts represented by different words or phrases—an ever frustrating thing. This is a barrier to understanding these fields. You must know certain ideas, words, and histories to understand the content.&lt;/p&gt;

&lt;p&gt;I have been collecting notes on these ideas and often revisit them to remind myself of what they are, what they mean, and why they exist. These notes were created for a myself, a layperson.&lt;/p&gt;

&lt;p&gt;In this series of forthcoming posts, I will write about concepts that I wish I knew better in a language that I can understand. I call this collection of posts &lt;em&gt;Letters To a Layperson&lt;/em&gt;, inspired by the phenomenal book &lt;em&gt;Letters to a Young Contrarian&lt;/em&gt; by Christopher Hitchens.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Chunking your csv</title>
      <link>/post/write-chunked-csv/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/write-chunked-csv/</guid>
      <description>


&lt;p&gt;Sometimes due to limitations of software, file uploads often have a row limit. I recently encountered this while creating texting campaigns using &lt;a href=&#34;relaytxt.io&#34;&gt;Relay&lt;/a&gt;. Relay is a peer-to-peer texting platform. It has a limitation of 20k contacts per texting campaign. This is a limitation when running a massive Get Out the Vote (GOTV) texting initiative.&lt;/p&gt;
&lt;p&gt;In order to solve this problem, a large csv must be split into multiple csv’s for upload. Though this could be solved with excel and Google Sheets, who wants to labor over that?&lt;/p&gt;
&lt;p&gt;Here I will go through the methodology of writing a csv into multiple. I will use data from the &lt;a href=&#34;https://www.google.com/search?q=quantitative+social+science&amp;amp;oq=quantitative+social+science&amp;amp;aqs=chrome..69i57j69i61j69i65j69i60l2j69i59.5035j0j1&amp;amp;sourceid=chrome&amp;amp;ie=UTF-8&#34;&gt;Quantitative Social Science&lt;/a&gt; book by Kosuke Imai.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)

social &amp;lt;- read_csv(&amp;quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&amp;quot;)

dim(social)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 305866      6&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This dataset has 305k observations and 6 columns. For this example let’s say we wanted to split this into files of 15,000 rows or fewer. We can use the following custom funciton:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;write_csv_chunk &amp;lt;- function(filepath, n, output_name) {
  df &amp;lt;- read_csv(filepath) # 1. read original file
  
  n_files &amp;lt;- ceiling(nrow(df)/n) # 2. identify how many files to make
  
  chunk_starts &amp;lt;- seq(1, n*n_files, by = n) #  3. identify the rown number to start on
  
  for (i in 1:n_files) { # 4. iterate through the csv to write the files
    chunk_end &amp;lt;- n*i # 4a
    df_to_write &amp;lt;- slice(df, chunk_starts[i]:chunk_end) # 4b
    fpath &amp;lt;- paste0(output_name, &amp;quot;_&amp;quot;, i, &amp;quot;.csv&amp;quot;) # 4c
    write_csv(df_to_write,  fpath) # 4d
    message(paste0(fpath, &amp;quot; was written.&amp;quot;)) # 4e
  }
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;The function has a few steps. Let’s walk through them. The step numbers are commented above.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Read in the csv.&lt;/li&gt;
&lt;li&gt;Identify the number of files that will have to be created.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This will be the number of rows of the data frame divided by the number of rows we want each file to have. This number will be rounded up to handle a remainder.&lt;/li&gt;
&lt;li&gt;In this case &lt;code&gt;ceiling(nrow(social) / 15000)&lt;/code&gt; is equal to 21.&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;3&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Identify the row number to begin splitting the dataframe for each file.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;This will be a factor of our &lt;code&gt;n&lt;/code&gt; plus 1, but will never exceed the &lt;code&gt;nrow(df)&lt;/code&gt;,&lt;/li&gt;
&lt;/ul&gt;
&lt;ol start=&#34;4&#34; style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;This is the fun part, writing our files. The number of iterations is the number of files.&lt;/li&gt;
&lt;/ol&gt;
&lt;ul&gt;
&lt;li&gt;&lt;em&gt;4a&lt;/em&gt;: The ending row number is the iteration number multiplied by the number of rows.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4b&lt;/em&gt;: use &lt;code&gt;slice()&lt;/code&gt; to cute the original data frame into the chunk beginning and chunk end&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4c&lt;/em&gt;: Creating the file paththat will be written.&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4d&lt;/em&gt;: Write the csv!&lt;/li&gt;
&lt;li&gt;&lt;em&gt;4e&lt;/em&gt;: Print a message about the file being printed.&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;soc_fpath &amp;lt;- &amp;quot;https://raw.githubusercontent.com/kosukeimai/qss/master/CAUSALITY/social.csv&amp;quot;
write_csv_chunk(filepath = soc_fpath, n = 25000, &amp;quot;../../static/data/chunk_data/social_chunked&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_1.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_2.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_3.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_4.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_5.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_6.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_7.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_8.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_9.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_10.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_11.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_12.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ../../static/data/chunk_data/social_chunked_13.csv was written.&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have these files split up, it will be good to know how to get them back into one piece! Check out my blog post on reading multiple csvs in as one data frame &lt;a href=&#34;josiahparry.com/post/read-chunked-csv&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Reading Multiple csvs as 1 data frame</title>
      <link>/post/read-chunked-csv/</link>
      <pubDate>Sat, 27 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/read-chunked-csv/</guid>
      <description>


&lt;p&gt;In an earlier &lt;a href=&#34;josiahparry.com/post/write-chunked-csv&#34;&gt;posting&lt;/a&gt; I wrote about having to break a single csv into multiple csvs. In other scenarios one data set maybe provided as multiple a csvs.&lt;/p&gt;
&lt;p&gt;Thankfully &lt;code&gt;purrr&lt;/code&gt; has a beautiful function called &lt;code&gt;map_df()&lt;/code&gt; which will make this into a two liner. This process has essentially 3 steps.&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Create a vector of all &lt;code&gt;.csv&lt;/code&gt; files that should be merged together.&lt;/li&gt;
&lt;li&gt;Read each file using &lt;code&gt;readr::read_csv()&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;Combine each dataframe into one.&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;code&gt;map_df()&lt;/code&gt; maps (applys) a function to each value of an object and produces a dataframe of all outputs.&lt;/p&gt;
&lt;p&gt;For this example I will use the csvs I created in a previous tutorial utilizing a dataset from the Quantitative Social Science book.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get all csv file names 
file_names &amp;lt;- list.files(&amp;quot;../../static/data/chunk_data&amp;quot;, pattern = &amp;quot;\\.csv&amp;quot;, full.names = TRUE)
file_names&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;../../static/data/chunk_data/social_chunked_1.csv&amp;quot; 
##  [2] &amp;quot;../../static/data/chunk_data/social_chunked_10.csv&amp;quot;
##  [3] &amp;quot;../../static/data/chunk_data/social_chunked_11.csv&amp;quot;
##  [4] &amp;quot;../../static/data/chunk_data/social_chunked_12.csv&amp;quot;
##  [5] &amp;quot;../../static/data/chunk_data/social_chunked_13.csv&amp;quot;
##  [6] &amp;quot;../../static/data/chunk_data/social_chunked_2.csv&amp;quot; 
##  [7] &amp;quot;../../static/data/chunk_data/social_chunked_3.csv&amp;quot; 
##  [8] &amp;quot;../../static/data/chunk_data/social_chunked_4.csv&amp;quot; 
##  [9] &amp;quot;../../static/data/chunk_data/social_chunked_5.csv&amp;quot; 
## [10] &amp;quot;../../static/data/chunk_data/social_chunked_6.csv&amp;quot; 
## [11] &amp;quot;../../static/data/chunk_data/social_chunked_7.csv&amp;quot; 
## [12] &amp;quot;../../static/data/chunk_data/social_chunked_8.csv&amp;quot; 
## [13] &amp;quot;../../static/data/chunk_data/social_chunked_9.csv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(tidyverse)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Attaching packages ──────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ✔ ggplot2 3.1.0     ✔ purrr   0.2.5
## ✔ tibble  1.4.2     ✔ dplyr   0.7.8
## ✔ tidyr   0.8.2     ✔ stringr 1.3.1
## ✔ readr   1.2.1     ✔ forcats 0.3.0&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## ── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──
## ✖ dplyr::filter() masks stats::filter()
## ✖ dplyr::lag()    masks stats::lag()&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# apply 
all_csvs &amp;lt;- map_df(file_names, read_csv)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )
## Parsed with column specification:
## cols(
##   sex = col_character(),
##   yearofbirth = col_double(),
##   primary2004 = col_double(),
##   messages = col_character(),
##   primary2006 = col_double(),
##   hhsize = col_double()
## )&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# preview the data
head(all_csvs)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 6
##   sex    yearofbirth primary2004 messages   primary2006 hhsize
##   &amp;lt;chr&amp;gt;        &amp;lt;dbl&amp;gt;       &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;            &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
## 1 male          1941           0 Civic Duty           0      2
## 2 female        1947           0 Civic Duty           0      2
## 3 male          1951           0 Hawthorne            1      3
## 4 female        1950           0 Hawthorne            1      3
## 5 female        1982           0 Hawthorne            1      3
## 6 male          1981           0 Control              0      3&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Coursera R-Programming: Week 2 Problems</title>
      <link>/post/tidy-coursera-r-programming/</link>
      <pubDate>Sat, 14 Apr 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/tidy-coursera-r-programming/</guid>
      <description>


&lt;p&gt;Over the past several weeks I have been helping students, career professionals, and people of other backgrounds learn R. During this time one this has become apparent, people are teaching the old paradigm of R and avoiding the tidyverse all together.&lt;/p&gt;
&lt;p&gt;I recently had a student reach out to me in need of help with the first programming assignment from the &lt;a href=&#34;https://www.coursera.org/learn/r-programming&#34;&gt;Coursera R-Programming&lt;/a&gt; course (part of the Johns Hopkins Data Science Specialization). This particular student was struggling with combining the her new knowledge of R data types, conditional statements, looping, control statements, scoping, and functions to solve the assignment problem set. I provided her with a walk through of each question in base R, the style of the course. I couldn’t help but empathize with her as I too learned the long way first. However I thought that she shouldn’t be learning the hard way first (see David Robinson’s &lt;a href=&#34;http://varianceexplained.org/r/teach-hard-way/&#34;&gt;blog post&lt;/a&gt;, &lt;em&gt;“Don’t teach students the hard way first”&lt;/em&gt;), she should be learning the effective way.&lt;/p&gt;
&lt;p&gt;In my written response to her, I gave her solutions to her problems in base R and using the tidyverse. Here, I will go over the problems and adress them from a tidy perspective. This will not serve as a full introduction to the tidyverse. For an introduction and a reason why the tidyverse is superior to base R, I leave you with &lt;a href=&#34;http://stat545.com/block009_dplyr-intro.html&#34;&gt;&lt;strong&gt;Stat 545&lt;/strong&gt;: &lt;em&gt;Introduction to dplyr&lt;/em&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;The assignment utilizes a directory of data called &lt;code&gt;specdata&lt;/code&gt; which can be downloaded &lt;a href=&#34;https://d396qusza40orc.cloudfront.net/rprog%2Fdata%2Fspecdata.zip&#34;&gt;&lt;strong&gt;here&lt;/strong&gt;&lt;/a&gt;, and describes it:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;The zip file contains 332 comma-separated-value (CSV) files containing pollution monitoring data for fine particulate matter (PM) air pollution at 332 locations in the United States. Each file contains data from a single monitor and the ID number for each monitor is contained in the file name. For example, data for monitor 200 is contained in the file “200.csv”. Each file contains three variables:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Date&lt;/strong&gt;: the date of the observation in &lt;code&gt;YYYY-MM-DD&lt;/code&gt; format (year-month-day)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;sulfate&lt;/strong&gt;: the level of sulfate PM in the air on that date (measured in micrograms per cubic meter)&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nitrate&lt;/strong&gt;: the level of nitrate PM in the air on that date (measured in micrograms per cubic meter)&lt;/li&gt;
&lt;/ul&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;For this programming assignment you will need to unzip this file and create the directory ‘specdata’. Once you have unzipped the zip file, do not make any modifications to the files in the ‘specdata’ directory. In each file you’ll notice that there are many days where either sulfate or nitrate (or both) are missing (coded as NA). This is common with air pollution monitoring data in the United States.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;hr /&gt;
&lt;div id=&#34;part-i&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part I&lt;/h1&gt;
&lt;p&gt;Problem 1:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write a function named ‘pollutantmean’ that calculates the mean of a pollutant (sulfate or nitrate) across a specified list of monitors. The function ‘pollutantmean’ takes three arguments: ‘directory’, ‘pollutant’, and ‘id’. Given a vector monitor ID numbers, ‘pollutantmean’ reads that monitors’ particulate matter data from the directory specified in the ‘directory’ argument and returns the mean of the pollutant across all of the monitors, ignoring any missing values coded as NA. A prototype of the function is as follows&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt; pollutantmean &amp;lt;- function(directory, pollutant, id = 1:332){
    ## &amp;#39;directory&amp;#39; is a character vector of length 1 indicating
    ## the location of the CSV files
    
    ## &amp;#39;pollutant&amp;#39; is a character vector of length 1 indicating
    ## the name of the pollutant for which we will calculate the 
    ## mean; either &amp;quot;sulfate&amp;quot; or &amp;quot;nitrate&amp;quot;

    ## &amp;#39;id&amp;#39; is an integer vector indicating the monitor ID numbers
    ## to be used

    ## Return the mean of the pollutant across all monitors list
    ## in the &amp;#39;id&amp;#39; vector (ignoring NA values)
    ## NOTE: Do not round the result!
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Before we tackle the function, I believe the best approach is to first solve the problem in a regular script. This problem has four clear steps:&lt;/p&gt;
&lt;ol style=&#34;list-style-type: decimal&#34;&gt;
&lt;li&gt;Identify files in the directory&lt;/li&gt;
&lt;li&gt;Subset files based on provided ID&lt;/li&gt;
&lt;li&gt;Read the files&lt;/li&gt;
&lt;li&gt;Calculate and return the mean on the desired column&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;This problem gives us a directory of files from which we need to read in the data based on the provided IDs. For the sake of this walk through we will randomly sample 10 values within the range designated in the problem statement (332).&lt;/p&gt;
&lt;p&gt;We will first generate random IDs, then identify all of the files within the specified directory and obtain their file paths using the &lt;code&gt;list.files()&lt;/code&gt; function. After this we will subset our file list based on the IDs, then iterate over our file list and read in each file as a csv using &lt;code&gt;purrr:map_df()&lt;/code&gt; combined with &lt;code&gt;readr::read_csv()&lt;/code&gt;. Fortunately &lt;code&gt;map_df()&lt;/code&gt; returns a nice and pretty data frame which lets us avoid having to explicitly bind each unique data frame.&lt;/p&gt;
&lt;div id=&#34;identify-files&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Identify Files&lt;/h4&gt;
&lt;p&gt;Here we create 10 random IDs and store them in the &lt;code&gt;ids&lt;/code&gt; variable. Next we use &lt;code&gt;list.files()&lt;/code&gt; to look within the &lt;code&gt;specdata&lt;/code&gt; folder that we downloaded above. Everyone’s path will most likely be different. Be sure to obtain the correct file path—help for &lt;a href=&#34;http://osxdaily.com/2013/06/19/copy-file-folder-path-mac-os-x/&#34;&gt;Mac&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;Next we identify the files we need based on the sampled &lt;code&gt;ids&lt;/code&gt; and store the subset in the &lt;code&gt;files_filtered&lt;/code&gt; variable. We use the values of the &lt;code&gt;ids&lt;/code&gt; to locate the file paths positionally. For example, ID number 1 is the first file, number 10 is the tenth, etc.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Load our handy dandy functions
library(tidyverse)

# 10 random IDs in ID range
ids &amp;lt;- sample(1:332, 10)

# Identify all files within the directory
files &amp;lt;- list.files(&amp;quot;../../data/specdata&amp;quot;, full.names = TRUE)

# Subset the data
files_filtered &amp;lt;- files[ids]

# View the files to verify
paste(ids, files_filtered)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##  [1] &amp;quot;302 ../../data/specdata/302.csv&amp;quot; &amp;quot;140 ../../data/specdata/140.csv&amp;quot;
##  [3] &amp;quot;326 ../../data/specdata/326.csv&amp;quot; &amp;quot;324 ../../data/specdata/324.csv&amp;quot;
##  [5] &amp;quot;59 ../../data/specdata/059.csv&amp;quot;  &amp;quot;104 ../../data/specdata/104.csv&amp;quot;
##  [7] &amp;quot;181 ../../data/specdata/181.csv&amp;quot; &amp;quot;147 ../../data/specdata/147.csv&amp;quot;
##  [9] &amp;quot;78 ../../data/specdata/078.csv&amp;quot;  &amp;quot;232 ../../data/specdata/232.csv&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;reading-the-files&#34; class=&#34;section level4&#34;&gt;
&lt;h4&gt;Reading the Files&lt;/h4&gt;
&lt;p&gt;Now that we have identified the files that we are going to read in, we can use &lt;code&gt;purrr:map_df()&lt;/code&gt; to apply the &lt;code&gt;readr::read_csv()&lt;/code&gt; function to each value of &lt;code&gt;files_filtered&lt;/code&gt; and return a data frame (hence the &lt;code&gt;_df()&lt;/code&gt; suffix). We supply additional arguments to &lt;code&gt;read_csv()&lt;/code&gt; to ensure that every column is read in properly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Read in the subset of the data. Notice the brackets after files[]
specdata &amp;lt;- map_df(files_filtered, read_csv, 
                   col_types = list(
                     col_date(),
                     col_double(),
                     col_double(),
                     col_integer()
                   ))

glimpse(specdata)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## Observations: 24,470
## Variables: 4
## $ Date    &amp;lt;date&amp;gt; 2001-01-01, 2001-01-02, 2001-01-03, 2001-01-04, 2001-...
## $ sulfate &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, 4.78, NA, NA, NA, NA, NA, NA, ...
## $ nitrate &amp;lt;dbl&amp;gt; NA, NA, NA, NA, NA, NA, 6.12, NA, NA, NA, NA, NA, NA, ...
## $ ID      &amp;lt;int&amp;gt; 302, 302, 302, 302, 302, 302, 302, 302, 302, 302, 302,...&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we get to utilize some &lt;code&gt;dplyr&lt;/code&gt; magic. Here we take the &lt;code&gt;specdata&lt;/code&gt; object we created from reading in our files, deselct the &lt;code&gt;Date&lt;/code&gt; column, then utilize &lt;code&gt;summarise_if()&lt;/code&gt; to apply the &lt;code&gt;mean()&lt;/code&gt; function to our data. &lt;code&gt;summarise_if()&lt;/code&gt; requires that we provide a logical statement as the first argument. &lt;em&gt;If&lt;/em&gt; (hence the &lt;code&gt;_if()&lt;/code&gt; suffix) the logical statement evaluates to &lt;code&gt;TRUE&lt;/code&gt; on a column then it will apply a list of functions to those columns where the statement evaluated to &lt;code&gt;TRUE&lt;/code&gt;. We can also specify additional arguments to the functions. Here we specify &lt;code&gt;na.rm = TRUE&lt;/code&gt; for handling missing values.&lt;/p&gt;
&lt;p&gt;In this case, we are checking to see if our columns are of the data type &lt;code&gt;double&lt;/code&gt; using the &lt;code&gt;is.double()&lt;/code&gt; function. If you’re wondering why we didn’t use &lt;code&gt;is.numeric()&lt;/code&gt;, it’s because the &lt;code&gt;ID&lt;/code&gt; column is an integer which is considered numeric.&lt;/p&gt;
&lt;p&gt;If we wanted to take the underlying vector of one of the columns, we can also, use &lt;code&gt;dplyr::pull(col_name)&lt;/code&gt;. This will be helpful later when we want to obtain the mean of just one column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Obtain mean nitrate
specdata %&amp;gt;% 
  select(-Date) %&amp;gt;% 
  summarise_if(is.double, mean, na.rm = TRUE) %&amp;gt;% 
  # Pull just the sulfate column
  pull(sulfate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.539072&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;specdata %&amp;gt;% 
  select(-Date) %&amp;gt;% 
  summarise_if(is.double, mean, na.rm = TRUE) %&amp;gt;% 
  # Pull just the nitrate column
  pull(nitrate)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 1.604677&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have all of the tools, we can put this together into a single function, which I will call &lt;code&gt;pollutant_mean()&lt;/code&gt; to somewhat adhere—functions should take the name of verbs—to the tidyverse style guide.&lt;/p&gt;
&lt;p&gt;Here we have three arguments:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;directory&lt;/code&gt;: Where to look for the files&lt;/li&gt;
&lt;li&gt;&lt;code&gt;pollutant&lt;/code&gt;: Which pollutant (nitrate or sulfate) to evaluate
&lt;ul&gt;
&lt;li&gt;This needs to be a character value unless we want to get into &lt;a href=&#34;https://dplyr.tidyverse.org/articles/programming.html&#34;&gt;tidyeval&lt;/a&gt;, which frankly I will leave to the professionals. But I will provide an alternative solution at the end that doesn’t require quoted &lt;code&gt;pollutant&lt;/code&gt; names.&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;id&lt;/code&gt;: Which monitoring stations we should look at&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Within the function we take everything we did in the above steps but generalize it to a function. We identify the files in the directory provided (&lt;code&gt;specdata&lt;/code&gt;), subset the files positionally based on the provided &lt;code&gt;id&lt;/code&gt; vector, and then iterate over the file names and read them in with &lt;code&gt;map_df()&lt;/code&gt; and &lt;code&gt;read_csv()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;Next we take our data and calculate the mean on both &lt;code&gt;sulfate&lt;/code&gt; and &lt;code&gt;nitrate&lt;/code&gt; columns. We then &lt;code&gt;pull()&lt;/code&gt; the specified column from the &lt;code&gt;pollutant&lt;/code&gt; argument and then &lt;code&gt;return()&lt;/code&gt; that value.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pollutant_mean &amp;lt;- function(directory, pollutant, id = 1:332) {
  files &amp;lt;- list.files(directory, full.names = TRUE)
  files_filtered &amp;lt;- files[id]
  specdata &amp;lt;- map_df(files_filtered, read_csv, 
                     col_types = list(
                       col_date(),
                       col_double(),
                       col_double(),
                       col_integer()
                     ))
  
  specdata %&amp;gt;% 
    select(-Date) %&amp;gt;% 
    summarise_if(is.double, mean, na.rm = TRUE) %&amp;gt;% 
    pull(pollutant) %&amp;gt;% 
    return()

}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Here we can test out the function with both types of pollutants and different &lt;code&gt;id&lt;/code&gt; values.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pollutant_mean(directory = &amp;quot;../../data/specdata&amp;quot;, pollutant = &amp;quot;sulfate&amp;quot;, id = sample(1:332, 20))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] 3.300254&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;#pollutant_mean(&amp;quot;../../data/specdata&amp;quot;, &amp;quot;nitrate&amp;quot;, 2)&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;part-ii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part II:&lt;/h1&gt;
&lt;p&gt;Let us continue to the second problem in the problem set.&lt;/p&gt;
&lt;p&gt;Problem 2:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write a function that reads a directory full of files and reports the number of completely observed cases in each data file. The function should return a data frame where the first column is the name of the file and the second column is the number of complete cases.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;The assignment provides an example function format, but I think it to be a bit misleading. So I will go about this in the way I think is best. We will work on creating a function called &lt;code&gt;complete_spec_cases()&lt;/code&gt; which will take only two arguments, &lt;code&gt;directory&lt;/code&gt;, and &lt;code&gt;id&lt;/code&gt;. &lt;code&gt;directory&lt;/code&gt; and &lt;code&gt;id&lt;/code&gt; will be used in the the same way as the previous problem.&lt;/p&gt;
&lt;p&gt;For this problem our goal is to identify how many complete cases there are by provided ID. This should be exceptionally simple. We will have to identify our files, subset them, and read them in the same way as before. Next we can identify complete cases by piping our &lt;code&gt;specdata&lt;/code&gt; object to &lt;code&gt;na.omit()&lt;/code&gt; which will remove any row with a missing value. Next, we have to group by the &lt;code&gt;ID&lt;/code&gt; column and pipe our grouped data frame to &lt;code&gt;count()&lt;/code&gt; which will count how many observations there are by group. We will then return this data frame to the user.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complete_spec_cases &amp;lt;- function(directory, id = 1:332) {

  files &amp;lt;- list.files(directory, full.names = TRUE)
  
  specdata &amp;lt;- map_df(files[id], read_csv,
                     col_types = list(
                       col_date(),
                       col_double(),
                       col_double(),
                       col_integer()
                     ))
  
  complete_specdata &amp;lt;- specdata %&amp;gt;% 
    na.omit() %&amp;gt;% 
    group_by(ID) %&amp;gt;% 
    summarise(nobs = n())
  
  return(complete_specdata)
}&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;complete_spec_cases(&amp;quot;../../data/specdata&amp;quot;, id = sample(1:332, 20))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 19 x 2
##       ID  nobs
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt;
##  1    11   443
##  2    44   283
##  3    48    62
##  4    77   345
##  5   116   806
##  6   127   428
##  7   171   614
##  8   182   465
##  9   184   816
## 10   198   858
## 11   209   151
## 12   249   230
## 13   272   253
## 14   290    91
## 15   302   937
## 16   306   203
## 17   314   888
## 18   326   215
## 19   327   162&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;div id=&#34;part-iii&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Part III:&lt;/h1&gt;
&lt;p&gt;This final problem is probably the most complicated, but with the method we just used above and with a bit more help from the &lt;code&gt;purrr&lt;/code&gt; and &lt;code&gt;dplyr&lt;/code&gt; packages, we can do this no problem.&lt;/p&gt;
&lt;p&gt;Problem 3:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Write a function that takes a directory of data files and a threshold for complete cases and calculates the correlation between sulfate and nitrate for monitor locations where the &lt;em&gt;number of completely observed cases&lt;/em&gt; (on all variables) is greater than the threshold. The function should return a vector of correlations for the monitors that meet the threshold requirement. If no monitors meet the threshold requirement, then the function should return a numeric vector of length 0. A prototype of this function follows:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;pre&gt;&lt;code&gt;Correct &amp;lt;- function(directory, threshold = 0){
    ## &amp;#39;directory&amp;#39; is a character vector of length 1 indicating
    ## the location of the CSV files

    ## &amp;#39;threshold&amp;#39; is a numeric vector of length 1 indicating the
    ## number of completely observed observations (on all
    ## variables) required to compute the correlation between
    ## nitrate and sulfate; the default is 0

    ## Return a numeric vector of correlations
    ## NOTE: Do not round the result!
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Let keep this simple. The above statement essentially is asking that we find the correlation between &lt;code&gt;nitrate&lt;/code&gt; and &lt;code&gt;sulfate&lt;/code&gt; for each monitoring station (ID). But there is a catch! Each ID must meet a specified threshold of complete cases, and if none of the monitors meet the requirement the function must return a &lt;code&gt;numeric(0)&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;The way we will structure this function will be to first read in the data—as we have done twice now, except this time there will be no subsetting of IDs. Then we need to identify the number of complete cases by ID—as we did in problem 2—and identify the stations that meet the threshold requirement. At this point we will use an &lt;code&gt;if&lt;/code&gt; statement to check if we have at least 1 monitoring station that meets our threshold, if we do not, we return the &lt;code&gt;numeric(0)&lt;/code&gt;—there is most likely a more tidy way to do this, but I am not aware. If we have at least 1 monitoring station that meets the specified threshold we will use an &lt;code&gt;inner_join()&lt;/code&gt; to make sure that &lt;code&gt;specdata&lt;/code&gt; contains only those IDs that meet the requirement.&lt;/p&gt;
&lt;p&gt;For the sake of this example, we will continue to use the &lt;code&gt;specdata&lt;/code&gt; object we created in previous examples, and we will set our threshold to &lt;code&gt;100.&lt;/code&gt; Once we identify the stations with the proper number of counts (&lt;code&gt;&amp;gt; 100&lt;/code&gt;), we will store that data frame in an object called &lt;code&gt;id_counts&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;id_counts &amp;lt;- specdata %&amp;gt;% 
    na.omit() %&amp;gt;% 
    group_by(ID) %&amp;gt;% 
    count() %&amp;gt;% 
    filter(n &amp;gt; 100) 
  
  if (nrow(id_counts) &amp;lt; 1) {
    return(numeric(0))
  } else {
    print(&amp;quot;All is well.&amp;quot;)
  }&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;All is well.&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;  specdata &amp;lt;- id_counts %&amp;gt;% 
    inner_join(specdata, by = &amp;quot;ID&amp;quot;) %&amp;gt;%
    na.omit() 
  
  specdata&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 4,138 x 5
## # Groups:   ID [9]
##       ID     n Date       sulfate nitrate
##    &amp;lt;int&amp;gt; &amp;lt;int&amp;gt; &amp;lt;date&amp;gt;       &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt;
##  1    59   445 2004-09-09    2.40   0.383
##  2    59   445 2004-09-12    1.45   0.383
##  3    59   445 2004-09-18    4.75   0.281
##  4    59   445 2004-09-24    9.47   0.623
##  5    59   445 2004-09-30    6.67   0.381
##  6    59   445 2004-10-03    2.90   0.326
##  7    59   445 2004-10-06    4.20   0.351
##  8    59   445 2004-10-09    2.41   0.539
##  9    59   445 2004-10-12    1.61   0.491
## 10    59   445 2004-10-15    1.95   0.306
## # ... with 4,128 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This is where it gets kind of funky. Once we have filtered down our data set, we need to calculate the correlations for each ID. The way that we do this is by nesting our data frame on the &lt;code&gt;ID&lt;/code&gt; column. Calling &lt;code&gt;nest(-ID)&lt;/code&gt; allows us to, for each value of ID, create a data frame for just those rows where the &lt;code&gt;ID&lt;/code&gt; is the same. We will then have a new list type column where each value is actually a data frame. Let’s check out what this looks like before we hop into the function.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;specdata %&amp;gt;% 
  nest(-ID)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
##      ID data              
##   &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;            
## 1    59 &amp;lt;tibble [445 × 4]&amp;gt;
## 2    78 &amp;lt;tibble [275 × 4]&amp;gt;
## 3   104 &amp;lt;tibble [385 × 4]&amp;gt;
## 4   140 &amp;lt;tibble [407 × 4]&amp;gt;
## 5   147 &amp;lt;tibble [302 × 4]&amp;gt;
## 6   181 &amp;lt;tibble [286 × 4]&amp;gt;
## 7   232 &amp;lt;tibble [886 × 4]&amp;gt;
## 8   302 &amp;lt;tibble [937 × 4]&amp;gt;
## 9   326 &amp;lt;tibble [215 × 4]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we know how to nest our data, we need to calculate the correlations for each row (ID value). We will do this by combining &lt;code&gt;mutate()&lt;/code&gt; and &lt;code&gt;map()&lt;/code&gt;. Here &lt;code&gt;.x&lt;/code&gt; references the data that is within each nested tibble. To learn more about &lt;code&gt;purrr&lt;/code&gt; I recommend the &lt;a href=&#34;http://r4ds.had.co.nz/iteration.html&#34;&gt;chapter&lt;/a&gt; on iteration from &lt;em&gt;R For Data Science&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;After we have done our calculations we undo our nesting using &lt;code&gt;unnest()&lt;/code&gt; on the new column we created, and deselect the &lt;code&gt;data&lt;/code&gt; column.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;specdata %&amp;gt;% 
  na.omit() %&amp;gt;% 
    nest(-ID) %&amp;gt;% 
    mutate(correlation = map(data, ~cor(.x$sulfate, .x$nitrate))) %&amp;gt;% 
  unnest(correlation) %&amp;gt;% 
  select(-data)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 9 x 2
##      ID correlation
##   &amp;lt;int&amp;gt;       &amp;lt;dbl&amp;gt;
## 1    59     0.0911 
## 2    78     0.00564
## 3   104    -0.129  
## 4   140     0.248  
## 5   147    -0.00120
## 6   181     0.0872 
## 7   232    -0.0745 
## 8   302     0.192  
## 9   326     0.140&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now place these above examples within a new function called &lt;code&gt;pollutant_cor()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pollutant_cor &amp;lt;- function(directory, threshold = 0) {
  files &amp;lt;- list.files(directory, full.names = TRUE)
  specdata &amp;lt;- map_df(files, read_csv, 
                     col_types = list(
                       col_date(),
                       col_double(),
                       col_double(),
                       col_integer()
                     )) %&amp;gt;% na.omit()
  
  id_counts &amp;lt;- specdata %&amp;gt;% 
    group_by(ID) %&amp;gt;% 
    count() %&amp;gt;% 
    filter(n &amp;gt; threshold) 
  
  if (nrow(id_counts) &amp;lt; 1) {
    return(numeric(0))
  }
  
  correlations &amp;lt;- id_counts %&amp;gt;% 
    inner_join(specdata, by = &amp;quot;ID&amp;quot;) %&amp;gt;% 
    nest(-ID) %&amp;gt;% 
    mutate(correlation = map(data, ~cor(.x$sulfate, .x$nitrate))) %&amp;gt;% 
    unnest(correlation)
  
  return(correlations)

  
}&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We can now test our function against two different thresholds to see how it reacts.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pollutant_cor(&amp;quot;../../data/specdata&amp;quot;, 100)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 254 x 3
##       ID data                 correlation
##    &amp;lt;int&amp;gt; &amp;lt;list&amp;gt;                     &amp;lt;dbl&amp;gt;
##  1     1 &amp;lt;tibble [117 × 4]&amp;gt;       -0.223 
##  2     2 &amp;lt;tibble [1,041 × 4]&amp;gt;     -0.0190
##  3     3 &amp;lt;tibble [243 × 4]&amp;gt;       -0.141 
##  4     4 &amp;lt;tibble [474 × 4]&amp;gt;       -0.0439
##  5     5 &amp;lt;tibble [402 × 4]&amp;gt;       -0.0682
##  6     6 &amp;lt;tibble [228 × 4]&amp;gt;       -0.124 
##  7     7 &amp;lt;tibble [442 × 4]&amp;gt;       -0.0759
##  8     8 &amp;lt;tibble [192 × 4]&amp;gt;       -0.160 
##  9     9 &amp;lt;tibble [275 × 4]&amp;gt;       -0.0868
## 10    10 &amp;lt;tibble [148 × 4]&amp;gt;        0.161 
## # ... with 244 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;If we set the threshold to 100,000, we should expect a &lt;code&gt;numeric(0)&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;pollutant_cor(&amp;quot;../../data/specdata&amp;quot;, 100000)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## numeric(0)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;It all works!&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>geniusR</title>
      <link>/project/geniusr/</link>
      <pubDate>Mon, 12 Feb 2018 00:00:00 +0000</pubDate>
      
      <guid>/project/geniusr/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Introducing geniusR</title>
      <link>/post/introducing-geniusr/</link>
      <pubDate>Sat, 27 Jan 2018 00:00:00 +0000</pubDate>
      
      <guid>/post/introducing-geniusr/</guid>
      <description>


&lt;div id=&#34;introducing-geniusr&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Introducing geniusR&lt;/h1&gt;
&lt;p&gt;&lt;code&gt;geniusR&lt;/code&gt; enables quick and easy download of song lyrics. The intent behind the package is to be able to perform text based analyses on songs in a tidy[text] format.&lt;/p&gt;
&lt;p&gt;This package was inspired by the release of Kendrick Lamar’s most recent album, &lt;strong&gt;DAMN.&lt;/strong&gt;. As most programmers do, I spent way too long to simplify a task, that being accessing song lyrics. Genius (formerly Rap Genius) is the most widly accessible platform for lyrics.&lt;/p&gt;
&lt;p&gt;The functions in this package enable easy access of individual song lyrics, album tracklists, and lyrics to whole albums.&lt;/p&gt;
&lt;div id=&#34;install-and-load-the-package&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Install and load the package&lt;/h2&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;devtools::install_github(&amp;quot;josiahparry/geniusR&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Load the package:&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;library(geniusR)
library(tidyverse) # For manipulation&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;getting-lyrics&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Getting Lyrics&lt;/h1&gt;
&lt;div id=&#34;whole-albums&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Whole Albums&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;genius_album()&lt;/code&gt; allows you to download the lyrics for an entire album in a &lt;code&gt;tidy&lt;/code&gt; format. There are two arguments &lt;code&gt;artists&lt;/code&gt; and &lt;code&gt;album&lt;/code&gt;. Supply the quoted name of artist and the album (if it gives you issues check that you have the album name and artists as specified on &lt;a href=&#34;https://genius.com&#34;&gt;Genius&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;This returns a tidy data frame with three columns:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;title&lt;/code&gt;: track name&lt;/li&gt;
&lt;li&gt;&lt;code&gt;track_n&lt;/code&gt;: track number&lt;/li&gt;
&lt;li&gt;&lt;code&gt;text&lt;/code&gt;: lyrics&lt;/li&gt;
&lt;/ul&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;emotions_math &amp;lt;- genius_album(artist = &amp;quot;Margaret Glaspy&amp;quot;, album = &amp;quot;Emotions and Math&amp;quot;)
emotions_math&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 371 x 3
##    title             track_n text                                  
##    &amp;lt;chr&amp;gt;               &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                 
##  1 Emotions and Math       1 Oh when I got you by my side          
##  2 Emotions and Math       1 Everything&amp;#39;s alright                  
##  3 Emotions and Math       1 Its just when your gone               
##  4 Emotions and Math       1 I start to snooze the alarm           
##  5 Emotions and Math       1 Cause I stay up until 4 in the morning
##  6 Emotions and Math       1 Counting all the days &amp;#39;til you&amp;#39;re back
##  7 Emotions and Math       1 Shivering in an ice cold bath         
##  8 Emotions and Math       1 Of emotions and math                  
##  9 Emotions and Math       1 Oh it&amp;#39;s a shame                       
## 10 Emotions and Math       1 And I&amp;#39;m to blame                      
## # ... with 361 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;multiple-albums&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Multiple Albums&lt;/h2&gt;
&lt;p&gt;If you wish to download multiple albums from multiple artists, try and keep it tidy and avoid binding rows if you can. We can achieve this in a tidy workflow by creating a tibble with two columns: &lt;code&gt;artist&lt;/code&gt; and &lt;code&gt;album&lt;/code&gt; where each row is an artist and their album. We can then iterate over those columns with &lt;code&gt;purrr:map2()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;In this example I will extract 3 albums from Kendrick Lamar and Sara Bareilles (two of my favotire musicians). The first step is to create the tibble with artists and album titles.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;albums &amp;lt;-  tibble(
  artist = c(
    rep(&amp;quot;Kendrick Lamar&amp;quot;, 3), 
    rep(&amp;quot;Sara Bareilles&amp;quot;, 3)
    ),
  album = c(
    &amp;quot;Section 80&amp;quot;, &amp;quot;Good Kid, M.A.A.D City&amp;quot;, &amp;quot;DAMN.&amp;quot;,
    &amp;quot;The Blessed Unrest&amp;quot;, &amp;quot;Kaleidoscope Heart&amp;quot;, &amp;quot;Little Voice&amp;quot;
    )
)

albums&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 2
##   artist         album                 
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                 
## 1 Kendrick Lamar Section 80            
## 2 Kendrick Lamar Good Kid, M.A.A.D City
## 3 Kendrick Lamar DAMN.                 
## 4 Sara Bareilles The Blessed Unrest    
## 5 Sara Bareilles Kaleidoscope Heart    
## 6 Sara Bareilles Little Voice&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;No we can iterate over each row using the &lt;code&gt;map2&lt;/code&gt; function. This allows us to feed each value from the &lt;code&gt;artist&lt;/code&gt; and &lt;code&gt;album&lt;/code&gt; columns to the &lt;code&gt;genius_album()&lt;/code&gt; function. Utilizing a &lt;code&gt;map&lt;/code&gt; call within a &lt;code&gt;dplyr::mutate()&lt;/code&gt; function creates a list column where each value is a &lt;code&gt;tibble&lt;/code&gt; with the data frame from &lt;code&gt;genius_album()&lt;/code&gt;. We will later unnest this.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;## We will have an additional artist column that will have to be dropped
album_lyrics &amp;lt;- albums %&amp;gt;% 
  mutate(tracks = map2(artist, album, genius_album))

album_lyrics&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 3
##   artist         album                  tracks              
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;                  &amp;lt;list&amp;gt;              
## 1 Kendrick Lamar Section 80             &amp;lt;tibble [1,184 × 3]&amp;gt;
## 2 Kendrick Lamar Good Kid, M.A.A.D City &amp;lt;tibble [2,192 × 3]&amp;gt;
## 3 Kendrick Lamar DAMN.                  &amp;lt;tibble [1,077 × 3]&amp;gt;
## 4 Sara Bareilles The Blessed Unrest     &amp;lt;tibble [666 × 3]&amp;gt;  
## 5 Sara Bareilles Kaleidoscope Heart     &amp;lt;tibble [582 × 3]&amp;gt;  
## 6 Sara Bareilles Little Voice           &amp;lt;tibble [577 × 3]&amp;gt;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now when you view this you will see that each value within the &lt;code&gt;tracks&lt;/code&gt; column is &lt;code&gt;&amp;lt;tibble&amp;gt;&lt;/code&gt;. This means that that value is infact another &lt;code&gt;tibble&lt;/code&gt;. We expand this using &lt;code&gt;tidyr::unnest()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Unnest the lyrics to expand 
lyrics &amp;lt;- album_lyrics %&amp;gt;% 
  unnest(tracks) %&amp;gt;%    # Expanding the lyrics 
  arrange(desc(artist)) # Arranging by artist name 

head(lyrics)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 6 x 5
##   artist         album              title track_n text                    
##   &amp;lt;chr&amp;gt;          &amp;lt;chr&amp;gt;              &amp;lt;chr&amp;gt;   &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                   
## 1 Sara Bareilles The Blessed Unrest Brave       1 You can be amazing      
## 2 Sara Bareilles The Blessed Unrest Brave       1 You can turn a phrase i…
## 3 Sara Bareilles The Blessed Unrest Brave       1 You can be the outcast  
## 4 Sara Bareilles The Blessed Unrest Brave       1 Or be the backlash of s…
## 5 Sara Bareilles The Blessed Unrest Brave       1 Or you can start speaki…
## 6 Sara Bareilles The Blessed Unrest Brave       1 Nothing&amp;#39;s gonna hurt yo…&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;song-lyrics&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Song Lyrics&lt;/h2&gt;
&lt;div id=&#34;genius_lyrics&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;&lt;code&gt;genius_lyrics()&lt;/code&gt;&lt;/h3&gt;
&lt;p&gt;Getting lyrics to a single song is pretty easy. Let’s get in our &lt;strong&gt;ELEMENT.&lt;/strong&gt; and checkout &lt;strong&gt;DNA.&lt;/strong&gt; by &lt;em&gt;Kendrick Lamar&lt;/em&gt;. But first, note that the &lt;code&gt;genius_lyrics()&lt;/code&gt; function takes two main arguments, &lt;code&gt;artist&lt;/code&gt; and &lt;code&gt;song&lt;/code&gt;. Be sure to spell the name of the artist and the song correctly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;DNA &amp;lt;- genius_lyrics(artist = &amp;quot;Kendrick Lamar&amp;quot;, song = &amp;quot;DNA.&amp;quot;)

DNA&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 95 x 3
##    title text                                                         line
##    &amp;lt;chr&amp;gt; &amp;lt;chr&amp;gt;                                                       &amp;lt;int&amp;gt;
##  1 DNA.  I got, I got, I got, I got—                                     1
##  2 DNA.  Loyalty, got royalty inside my DNA                              2
##  3 DNA.  Cocaine quarter piece, got war and peace inside my DNA          3
##  4 DNA.  I got power, poison, pain and joy inside my DNA                 4
##  5 DNA.  I got hustle though, ambition flow inside my DNA                5
##  6 DNA.  I was born like this, since one like this, immaculate conc…     6
##  7 DNA.  I transform like this, perform like this, was Yeshua new w…     7
##  8 DNA.  I don&amp;#39;t contemplate, I meditate, then off your fucking head     8
##  9 DNA.  This that put-the-kids-to-bed                                   9
## 10 DNA.  This that I got, I got, I got, I got—                          10
## # ... with 85 more rows&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;This returns a &lt;code&gt;tibble&lt;/code&gt; with three columns &lt;code&gt;title&lt;/code&gt;, &lt;code&gt;text&lt;/code&gt;, and &lt;code&gt;line&lt;/code&gt;. However, you can specifiy additional arguments to control the amount of information to be returned using the &lt;code&gt;info&lt;/code&gt; argument.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;info = &amp;quot;title&amp;quot;&lt;/code&gt; (default): Return the lyrics, line number, and song title.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;info = &amp;quot;simple&amp;quot;&lt;/code&gt;: Return just the lyrics and line number.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;info = &amp;quot;artist&amp;quot;&lt;/code&gt;: Return the lyrics, line number, and artist.&lt;/li&gt;
&lt;li&gt;&lt;code&gt;info = &amp;quot;all&amp;quot;&lt;/code&gt;: Return lyrics, line number, song title, artist.&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;tracklists&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Tracklists&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;genius_tracklist()&lt;/code&gt;, given an &lt;code&gt;artist&lt;/code&gt; and an &lt;code&gt;album&lt;/code&gt; will return a barebones &lt;code&gt;tibble&lt;/code&gt; with the track title, track number, and the url to the lyrics.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genius_tracklist(artist = &amp;quot;Basement&amp;quot;, album = &amp;quot;Colourmeinkindness&amp;quot;) &lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 10 x 3
##    title     track_n track_url                                   
##    &amp;lt;chr&amp;gt;       &amp;lt;int&amp;gt; &amp;lt;chr&amp;gt;                                       
##  1 Whole           1 https://genius.com/Basement-whole-lyrics    
##  2 Covet           2 https://genius.com/Basement-covet-lyrics    
##  3 Spoiled         3 https://genius.com/Basement-spoiled-lyrics  
##  4 Pine            4 https://genius.com/Basement-pine-lyrics     
##  5 Bad Apple       5 https://genius.com/Basement-bad-apple-lyrics
##  6 Breathe         6 https://genius.com/Basement-breathe-lyrics  
##  7 Control         7 https://genius.com/Basement-control-lyrics  
##  8 Black           8 https://genius.com/Basement-black-lyrics    
##  9 Comfort         9 https://genius.com/Basement-comfort-lyrics  
## 10 Wish           10 https://genius.com/Basement-wish-lyrics&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;nitty-gritty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Nitty Gritty&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;genius_lyrics()&lt;/code&gt; generates a url to Genius which is fed to &lt;code&gt;genius_url()&lt;/code&gt;, the function that does the heavy lifting of actually fetching lyrics.&lt;/p&gt;
&lt;p&gt;I have not figured out all of the patterns that are used for generating the Genius.com urls, so errors are bound to happen. If &lt;code&gt;genius_lyrics()&lt;/code&gt; returns an error. Try utilizing &lt;code&gt;genius_tracklist()&lt;/code&gt; and &lt;code&gt;genius_url()&lt;/code&gt; together to get the song lyrics.&lt;/p&gt;
&lt;p&gt;For example, say “(No One Knows Me) Like the Piano” by &lt;em&gt;Sampha&lt;/em&gt; wasn’t working in a standard &lt;code&gt;genius_lyrics()&lt;/code&gt; call.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;piano &amp;lt;- genius_lyrics(&amp;quot;Sampha&amp;quot;, &amp;quot;(No One Knows Me) Like the Piano&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We could grab the tracklist for the album &lt;em&gt;Process&lt;/em&gt; which the song is from. We could then isolate the url for &lt;em&gt;(No One Knows Me) Like the Piano&lt;/em&gt; and feed that into `genius_url().&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Get the tracklist for 
process &amp;lt;- genius_tracklist(&amp;quot;Sampha&amp;quot;, &amp;quot;Process&amp;quot;)

# Filter down to find the individual song
piano_info &amp;lt;- process %&amp;gt;% 
  filter(title == &amp;quot;(No One Knows Me) Like the Piano&amp;quot;)

# Filter song using string detection
# process %&amp;gt;% 
#  filter(stringr::str_detect(title, coll(&amp;quot;Like the piano&amp;quot;, ignore_case = TRUE)))

piano_url &amp;lt;- piano_info$track_url&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Now that we have the url, feed it into &lt;code&gt;genius_url()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genius_url(piano_url, info = &amp;quot;simple&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## # A tibble: 13 x 1
##    text                                                                   
##    &amp;lt;chr&amp;gt;                                                                  
##  1 No one knows me like the piano in my mother&amp;#39;s home                     
##  2 You would show me I had something some people call a soul              
##  3 And you dropped out the sky, oh you arrived when I was three years old 
##  4 No one knows me like the piano in my mother&amp;#39;s home                     
##  5 You know I left, I flew the nest                                       
##  6 And you know I won&amp;#39;t be long                                           
##  7 And in my chest you know me best                                       
##  8 And you know I&amp;#39;ll be back home                                         
##  9 An angel by her side, all of the times I knew we couldn&amp;#39;t cope         
## 10 They said that it&amp;#39;s her time, no tears in sight, I kept the feelings c…
## 11 And you took hold of me and never, never, never let me go              
## 12 &amp;#39;Cause no one knows me like the piano in my mother&amp;#39;s home              
## 13 In my mother&amp;#39;s home&lt;/code&gt;&lt;/pre&gt;
&lt;hr /&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;on-the-internals&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;On the Internals&lt;/h1&gt;
&lt;div id=&#34;generative-functions&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Generative functions&lt;/h2&gt;
&lt;p&gt;This package works almost entirely on pattern detection. The urls from &lt;em&gt;Genius&lt;/em&gt; are (mostly) easily reproducible (shout out to &lt;a href=&#34;https://twitter.com/CivicAngela&#34;&gt;Angela Li&lt;/a&gt; for pointing this out).&lt;/p&gt;
&lt;p&gt;The two functions that generate urls are &lt;code&gt;gen_song_url()&lt;/code&gt; and &lt;code&gt;gen_album_url()&lt;/code&gt;. To see how the functions work, try feeding an artist and song title to &lt;code&gt;gen_song_url()&lt;/code&gt; and an artist and album title to &lt;code&gt;gen_album_url()&lt;/code&gt;.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_song_url(&amp;quot;Laura Marling&amp;quot;, &amp;quot;Soothing&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://genius.com/Laura-Marling-Soothing-lyrics&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;gen_album_url(&amp;quot;Daniel Caesar&amp;quot;, &amp;quot;Freudian&amp;quot;)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## [1] &amp;quot;https://genius.com/albums/Daniel-Caesar/Freudian&amp;quot;&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;code&gt;genius_lyrics()&lt;/code&gt; calls &lt;code&gt;gen_song_url()&lt;/code&gt; and feeds the output to &lt;code&gt;genius_url()&lt;/code&gt; which preforms the scraping.&lt;/p&gt;
&lt;p&gt;Getting lyrics for albums is slightly more involved. It first calls &lt;code&gt;genius_tracklist()&lt;/code&gt; which first calls &lt;code&gt;gen_album_url()&lt;/code&gt; then using the handy package &lt;code&gt;rvest&lt;/code&gt; scrapes the song titles, track numbers, and song lyric urls. Next, the song urls from the output are iterated over and fed to &lt;code&gt;genius_url()&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;To make this more clear, take a look inside of &lt;code&gt;genius_album()&lt;/code&gt;&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;genius_album &amp;lt;- function(artist = NULL, album = NULL, info = &amp;quot;simple&amp;quot;) {

  # Obtain tracklist from genius_tracklist
  album &amp;lt;- genius_tracklist(artist, album) %&amp;gt;%

    # Iterate over the url to the song title
    mutate(lyrics = map(track_url, genius_url, info)) %&amp;gt;%

    # Unnest the tibble with lyrics
    unnest(lyrics) %&amp;gt;%
    
    # Deselect the track url
    select(-track_url)


  return(album)
}&lt;/code&gt;&lt;/pre&gt;
&lt;div id=&#34;notes&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Notes:&lt;/h3&gt;
&lt;p&gt;As this is my first &lt;em&gt;“package”&lt;/em&gt; there will be many issues. Please submit an issue and I will do my best to attend to it.&lt;/p&gt;
&lt;p&gt;There are already issues of which I am present (the lack of error handling). If you would like to take those on, please go ahead and make a pull request. Please contact me on &lt;a href=&#34;twitter.com/josiahparry&#34;&gt;Twitter&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
